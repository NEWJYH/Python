{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "577a3852",
   "metadata": {},
   "source": [
    "수업 목표\n",
    "\n",
    "머신러닝을 이해하기 위한 핵심개념을 파악합니다.\n",
    "\n",
    "데이터로부터 Feature를 만드는 과정을 이해합니다.\n",
    "\n",
    "Feature를 만들 때 도메인 전문성의 중요성을 이해합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace2910c",
   "metadata": {},
   "source": [
    "실습 목표\n",
    "\n",
    "Data Preprocessing이 어떤 기능을 하는지 이해합니다.\n",
    "\n",
    "Feature Engineering이 어떤 기능을 하는지 이해합니다.\n",
    "\n",
    "도메인 지식을 활용해 생성한 Feature가 머신러닝 모델의 성능을\n",
    "어떻게 변화시키는지 이해합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb04ee",
   "metadata": {},
   "source": [
    "목 차\n",
    "\n",
    "1. 머신러닝을 위한 핵심개념 살펴보기\n",
    "\n",
    "2. 머신러닝을 위한 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa63dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed3abe8e",
   "metadata": {},
   "source": [
    "머신러닝을 위한 핵심개념 살펴보기 (1)\n",
    "\n",
    "Data, Feature, Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa0199",
   "metadata": {},
   "source": [
    "1. 머신러닝을 위한 핵심개념 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fb909",
   "metadata": {},
   "source": [
    "Data\n",
    "\n",
    "현실 시계의 어떤 현상을\n",
    "\n",
    "관찰하여\n",
    "\n",
    "기록한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ab76e9",
   "metadata": {},
   "source": [
    "Heart Data\n",
    "\n",
    "심장 상태를 \n",
    "\n",
    "관찰하여\n",
    "\n",
    "기록한 것\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d439fd",
   "metadata": {},
   "source": [
    "Data\n",
    "\n",
    "거대한 퍼즐의 조각"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a63571",
   "metadata": {},
   "source": [
    "데이터의 특징\n",
    "\n",
    "Facts : 객관화된 자료\n",
    "\n",
    "No meaning : 데이터 자체에는 의미가 없다.\n",
    "\n",
    "Representation of real world : 실제 세상을 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f3da99",
   "metadata": {},
   "source": [
    "Feature(요인, 변수)\n",
    "\n",
    "Feature는 데이터(data)를 컴퓨터가 이해할 수 있도록\n",
    "수치(numeric)또는 디지털(digitized)로 표현/표상(representation)한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2239ee",
   "metadata": {},
   "source": [
    "Titanic Feature\n",
    "\n",
    "타이타닉호 티켓을    디지털 형태로 컴퓨터가 이해하도록    표현/표상한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da1fed",
   "metadata": {},
   "source": [
    "Synonym of \"Feature\"(기능의 동의어)\n",
    "\n",
    "- Independent Variable # 독립 변수\n",
    "- Explanatory Variable # 설명 변수\n",
    "- Predictor # 예측 변수\n",
    "- Input # 입력\n",
    "- Attribute # 속성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4605043c",
   "metadata": {},
   "source": [
    "Target\n",
    "\n",
    "예측하려는 목표\n",
    "Label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda13183",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edf9f917",
   "metadata": {},
   "source": [
    "머신러닝을 위한 핵심개념 살펴보기 (2)\n",
    "\n",
    "Model, Algorithm, Error, Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d8b292",
   "metadata": {},
   "source": [
    "Model ?\n",
    "\n",
    "실제의 무엇을 더 작게 추상화 된 형태로 표현한 것\n",
    "모형 또는 본보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c84d2",
   "metadata": {},
   "source": [
    "Machine Learning Model\n",
    "\n",
    "어떠한 문제를 해결하기 위해 수립한 가설을\n",
    "논리적, 수학적 함수식의 형태로 표현한 것\n",
    "\n",
    "Input -> f(x) -> Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb7c94",
   "metadata": {},
   "source": [
    "Algorithm\n",
    "\n",
    "입력된 자료를 바탕으로 원하는 결과를 유도하기 위해\n",
    "일련의 논리적인 순서와 절차를 규칙화한 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a6aade",
   "metadata": {},
   "source": [
    "Machine Learning Algorithm\n",
    "\n",
    "Model이 어떠한 문제를 해결하기 위한 함수식이라면\n",
    "\n",
    "Algorithm은 그 함수식을 만들어내는 일련의 절차, 규칙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d034c77a",
   "metadata": {},
   "source": [
    "Loss, Cost, Error\n",
    "\n",
    "예측 목표로부터 예측 결과의 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e320db36",
   "metadata": {},
   "source": [
    "Learning\n",
    "\n",
    "예측 목표로부터 예측 결과의 오차를 최소화하는 함수식을 찾아내는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e2e79e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87f14302",
   "metadata": {},
   "source": [
    "데이터 준비의 중요성과 파이프라인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7bcac",
   "metadata": {},
   "source": [
    "2. 머신러닝을 위한 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56227f3",
   "metadata": {},
   "source": [
    "Think First,\n",
    "\n",
    "Gabage Input → Gabage Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d6882f",
   "metadata": {},
   "source": [
    "Data Preparation\n",
    "\n",
    "Source1\n",
    "Source2 ---> Data --> Data Preprocessing  --> Features ->Learning\n",
    "Source3               Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f1f2c",
   "metadata": {},
   "source": [
    "Data Preparation Pipeline\n",
    "\n",
    "Data\n",
    " ↓\n",
    "Data           Data             Feature\n",
    "Acquisition -> Preprocessing -> Engineering -> Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4696d5",
   "metadata": {},
   "source": [
    "Feature Engineering : 가공 도메인에 도움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33008765",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd256c15",
   "metadata": {},
   "source": [
    "머신러닝 프로젝트 성공의 열쇠\n",
    "\n",
    "대다수의 Data Preprocessing과 Feature Engineering 기법은\n",
    "도메인에 많은 영향을 받습니다. (Domain Specific)\n",
    "\n",
    "꾸겨진 종이 -- Data Preprocessing Feature Engineering--> 펴진 종이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b11fbb8",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "\n",
    "컴퓨터가 좀 더 잘 받아들일 수 있는 형태로\n",
    "Data를 가공하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6c08a",
   "metadata": {},
   "source": [
    "Techniques of Data Preprocessing\n",
    "\n",
    "- Vectorization : 데이터를 백터로 수학적으로 표현\n",
    "- Normalization : 표준화 작업\n",
    "- Handling missing Values : 결측값을 제거 또는 채워서 사용할지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebfd7ef",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "\n",
    "도메인 지식을 활용하여\n",
    "머신러닝 알고리즘이 학습을 잘 진행할 수 있도록\n",
    "Preprocessed Data를 변환하는 작업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fa809",
   "metadata": {},
   "source": [
    "Techniques of Feature Engineering\n",
    "\n",
    "- Feature Transformation : 변환\n",
    "- Feature Generation : 새롭게\n",
    "- Feature Selection : 선택\n",
    "- Feature Extraction : 추출\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0554a20",
   "metadata": {},
   "source": [
    "Types of Feature\n",
    "\n",
    "Numerical : Age, Height, Price : 숫자\n",
    " \n",
    "Categorical : Gender, Class, Job : 범주"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b46fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b746155",
   "metadata": {},
   "source": [
    "데이터 설명\n",
    "컬럼명\t설명\n",
    "survived\t생존 여부\n",
    "1(생존), 0(사망)\n",
    "pclass\t좌석 클래스\n",
    "1(1등석), 2(2등석), 3(3등석)\n",
    "name\t이름\n",
    "sex\t성별\n",
    "female(여성), male(남성)\n",
    "age\t나이\n",
    "sibsp\t함께 탑승한 형제 또는 배우자의 수\n",
    "parch\t함께 탑승한 부모 또는 자녀의 수\n",
    "ticket\t티켓 번호\n",
    "fare\t티켓 요금\n",
    "cabin\t선실 번호\n",
    "embarked\t탑승한 곳\n",
    "C(Cherbourg), Q(Queenstown), S(Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a815f",
   "metadata": {},
   "source": [
    "데이터 준비가 필요한 타이타닉 데이터\n",
    "이번 실습에 사용할 타이타닉 데이터입니다.\n",
    "아래의 데이터는 현재 Data Preprocessing과 Feature Engineering의 데이터 준비가 필요합니다.\n",
    "\n",
    "왼쪽의 데이터 설명을 참고하여 데이터의 각 feature 값에 어떠한 준비가 필요할지 생각해보며 데이터를 살펴보세요.\n",
    "\n",
    "survived\tpclass\tname\tsex\tage\tsibsp\tparch\tticket\tfare\tcabin\tembarked\n",
    "1\t2\t“Brown, Miss. Amelia “”Mildred”””\tfemale\t24.0\t0\t0\t248733\t13.0\tF33\tS\n",
    "0\t2\tChapman, Mr. Charles Henry\tmale\t52.0\t0\t0\t248731\t13.5\tNaN\tS\n",
    "1\t1\tFortune, Miss. Mabel Helen\tfemale\t23.0\t3\t2\t19950\t263.0\tC23 C25 C27\tS\n",
    "0\t3\tLievens, Mr. Rene Aime\tmale\t24.0\t0\t0\t345781\t9.5\tNaN\tS\n",
    "1\t3\tBaclini, Miss. Helene Barbara\tfemale\t0.75\t2\t1\t2666\t19.2583\tNaN\tC\n",
    "0\t3\tHenry, Miss. Delia\tfemale\tNaN\t0\t0\t382649\t7.75\tNaN\tQ\n",
    "0\t3\tHart, Mr. Henry\tmale\tNaN\t0\t0\t394140\t6.8583\tNaN\tQ\n",
    "1\t1\tBidois, Miss. Rosalie\tfemale\t42.0\t0\t0\tPC 17757\t227.525\tNaN\tC\n",
    "1\t1\tDaniel, Mr. Robert Williams\tmale\t27.0\t0\t0\t113804\t30.5\tNaN\tS\n",
    "1\t1\tAnderson, Mr. Harry\tmale\t48.0\t0\t0\t19952\t26.55\tE12\tS\n",
    "0\t3\tSkoog, Miss. Margit Elizabeth\tfemale\t2.0\t3\t2\t347088\t27.9\tNaN\tS\n",
    "0\t3\tBurke, Mr. Jeremiah\tmale\t19.0\t0\t0\t365222\t6.75\tNaN\tQ\n",
    "0\t3\tDanbom, Mr. Ernst Gilbert\tmale\t34.0\t1\t1\t347080\t14.4\tNaN\tS\n",
    "0\t3\tScanlan, Mr. James\tmale\tNaN\t0\t0\t36209\t7.725\tNaN\tQ\n",
    "0\t3\tCalic, Mr. Petar\tmale\t17.0\t0\t0\t315086\t8.6625\tNaN\tS\n",
    "0\t1\tClifford, Mr. George Quincy\tmale\tNaN\t0\t0\t110465\t52.0\tA14\tS\n",
    "0\t1\tHarrison, Mr. William\tmale\t40.0\t0\t0\t112059\t0.0\tB94\tS\n",
    "1\t3\tDorking, Mr. Edward Arthur\tmale\t19.0\t0\t0\tA/5. 10482\t8.05\tNaN\tS\n",
    "1\t3\tBaclini, Miss. Eugenie\tfemale\t0.75\t2\t1\t2666\t19.2583\tNaN\tC\n",
    "0\t3\tAsplund, Master. Clarence Gustaf Hugo\tmale\t9.0\t4\t2\t347077\t31.3875\tNaN\tS\n",
    "1\t3\tMoor, Master. Meier\tmale\t6.0\t0\t1\t392096\t12.475\tE121\tS\n",
    "0\t3\tKink, Mr. Vincenz\tmale\t26.0\t2\t0\t315151\t8.6625\tNaN\tS\n",
    "0\t3\tJonkoff, Mr. Lalio\tmale\t23.0\t0\t0\t349204\t7.8958\tNaN\tS\n",
    "1\t1\tPeuchen, Major. Arthur Godfrey\tmale\t52.0\t0\t0\t113786\t30.5\tC104\tS\n",
    "0\t3\tCook, Mr. Jacob\tmale\t43.0\t0\t0\tA/5 3536\t8.05\tNaN\tS\n",
    "1\t1\tLines, Miss. Mary Conover\tfemale\t16.0\t0\t1\tPC 17592\t39.4\tD28\tS\n",
    "1\t1\tGoldenberg, Mr. Samuel L\tmale\t49.0\t1\t0\t17453\t89.1042\tC92\tC\n",
    "0\t2\tPengelly, Mr. Frederick William\tmale\t19.0\t0\t0\t28665\t10.5\tNaN\tS\n",
    "1\t3\tMcGovern, Miss. Mary\tfemale\tNaN\t0\t0\t330931\t7.8792\tNaN\tQ\n",
    "1\t2\tGarside, Miss. Ethel\tfemale\t34.0\t0\t0\t243880\t13.0\tNaN\tS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53add876",
   "metadata": {},
   "source": [
    "도메인 지식 없이\n",
    "Data Preprocessing 하기\n",
    "도메인 지식이 없더라도 Data Preprocessing 을 할 수 있습니다. 어떤 경우에는 일부러 데이터에 대한 이해 없이 기계적으로 작업을 하기도 합니다.\n",
    "\n",
    "따라서 이번 실습에서는 도메인 지식 없이 데이터를 준비해보며 다음의 4가지 Data Preprocessing 과정을 타이타닉 데이터에 직접 적용해보도록 하겠습니다.\n",
    "\n",
    "1) Check_missing_values (결측치 확인)\n",
    "\n",
    "2) Variable Selection (변수 선택)\n",
    "\n",
    "3) Handling Missing Values (결측 데이터 처리)\n",
    "\n",
    "4) Vectorization (데이터 변환)\n",
    "\n",
    "데이터 설명\n",
    "컬럼명\t설명\n",
    "survived\t생존 여부\n",
    "1(생존), 0(사망)\n",
    "pclass\t좌석 클래스\n",
    "1(1등석), 2(2등석), 3(3등석)\n",
    "name\t이름\n",
    "sex\t성별\n",
    "female(여성), male(남성)\n",
    "age\t나이\n",
    "sibsp\t함께 탑승한 형제 또는 배우자의 수\n",
    "parch\t함께 탑승한 부모 또는 자녀의 수\n",
    "ticket\t티켓 번호\n",
    "fare\t티켓 요금\n",
    "cabin\t선실 번호\n",
    "embarked\t탑승한 곳\n",
    "C(Cherbourg), Q(Queenstown), S(Southampton)\n",
    "\n",
    "지시사항\n",
    "데이터 준비 과정을 실습하기 전,\n",
    "먼저 실행 버튼을 눌러 데이터를 확인해보세요.\n",
    "\n",
    "1) Check_missing_values 실습 방법\n",
    "아래의 코드를 입력하고 실행 버튼을 눌러 feature 별 데이터 결측치 비율을 확인하세요.\n",
    "\n",
    "check_missing_values(titanic)\n",
    "Copy\n",
    "2) Variable Selection 실습 방법\n",
    "아래의 코드를 입력하고 실행 버튼을 눌러 데이터 처리 결과를 확인하세요.\n",
    "\n",
    "variable_selection(titanic)\n",
    "Copy\n",
    "결측치가 많아 제거되는 컬럼 : Cabin\n",
    "불필요한 컬럼 : Name, Ticket\n",
    "3) Handling Missing Values 실습 방법\n",
    "아래의 코드를 입력하고 실행 버튼을 눌러 데이터 처리 결과를 확인하세요.\n",
    "handling_missing_values(titanic)\n",
    "Copy\n",
    "나이(Age)의 결측치를 중앙값(‘median’)으로 채웁니다.\n",
    "4) Vectorization 실습 방법\n",
    "아래의 코드를 입력하고 실행 버튼을 눌러 데이터 처리 결과를 확인하세요.\n",
    "\n",
    "vectorization_sex : 성별을 변환\n",
    "vectorization_embarked : 탑승한 곳을 변환\n",
    "vectorization_sex(titanic)\n",
    "vectorization_embarked(titanic)\n",
    "Copy\n",
    "추천\n",
    "여러 실습 방법을 조합해서 실행해보아도 좋습니다\n",
    "\n",
    "variable_selection(titanic)\n",
    "vectorization_sex(titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main.py\n",
    "from without_domain import *\n",
    "\n",
    "def data_preparation_without_domain():\n",
    "    titanic = load_titanic_dataset()\n",
    "    \n",
    "    # [데이터 준비 방법]\n",
    "    \n",
    "    # 1. Check_missing_values\n",
    "    \n",
    "    # 데이터 내의 결측치 비율을 확인합니다.\n",
    "    # 일반적으로 데이터에 결측치가 있으면 모델이 정상적으로 작동하지 않기 때문에\n",
    "    # 컬럼 별로 결측치가 얼마나 있는지 비율을 확인하는 것이 중요합니다.\n",
    "    \n",
    "    # 2. Variable Selection\n",
    "    \n",
    "    # 결측치가 많고 불필요한 컬럼은 제거합니다.\n",
    "    # 도메인에 따라 다르지만 보통 한 변수내의 데이터 결측치 비율이 50% 이상일 경우,\n",
    "    # 해당 변수 자체를 제거하는 것이 좋습니다.\n",
    "\n",
    "    # 3. Handling Missing Values\n",
    "    \n",
    "    # 데이터 내에 결측치가 있는 경우 다른 대체값으로 결측치를 채웁니다.\n",
    "    # 예를 들어, `Age` feature 내의 결측치를 나이의 중앙값(‘median’)으로 채워줍니다.\n",
    "    \n",
    "    # 4. Vectorization\n",
    "    \n",
    "    # 컴퓨터가 이해할 수 있도록 데이터를 변환합니다.\n",
    "    # 예를 들어, `sex` feature 값인 ‘female’, 'male’은 사람은 이해할 수 있지만,\n",
    "    # 컴퓨터는 여성인지 남성인지 알 수 없습니다.\n",
    "    # 따라서, 컴퓨터가 이해할 수 있도록 male은 0으로, female은 1로 변환합니다.\n",
    "    \n",
    "    '''\n",
    "    # [실습] \n",
    "    \n",
    "    \n",
    "    아래에 실습 코드를 입력하여\n",
    "    데이터가 어떻게 처리되는지 확인하세요\n",
    "    '''\n",
    "    \n",
    "    check_missing_values(titanic)\n",
    "    variable_selection(titanic)\n",
    "    handling_missing_values(titanic)\n",
    "    vectorization_sex(titanic)\n",
    "    vectorization_embarked(titanic)\n",
    "    # 데이터 처리 결과를 보여줍니다\n",
    "    show_result(titanic)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tdata_preparation_without_domain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without_domain.py\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "#import elice_utils\n",
    "pd.set_option('display.max_columns', 20)\n",
    "\n",
    "def load_titanic_dataset():\n",
    "    print('>>> 타이타닉 데이터셋을 불러옵니다...')\n",
    "    elice_utils.send_image(\"./data/image01.png\")\n",
    "    sleep(2)\n",
    "    print(' ')\n",
    "    df = pd.read_csv('./data/titanic.csv')\n",
    "    df = df.drop('PassengerId', axis=1)\n",
    "    return df\n",
    "\t\n",
    "def check_missing_values(df):\n",
    "    if df is None or df.empty:\n",
    "        print('타이타닉 데이터를 입력해주세요')\n",
    "        return None\n",
    "    titanic_null = df.isnull().sum()\n",
    "    print(\"\\n>> 타이타닉 데이터의 변수별 결측치 비율은 다음과 같습니다.\")\n",
    "    sleep(2)\n",
    "    for col, val in titanic_null.items():\n",
    "        print(\"{} : {:.2f}%\".format(col, val/df.shape[0]*100))\n",
    "\n",
    "def variable_selection(df):\n",
    "    if df is None or df.empty:\n",
    "        print('타이타닉 데이터를 입력해주세요')\n",
    "        return None\n",
    "    columns = ['Name', 'Ticket', 'Cabin']\n",
    "    df.drop(columns, axis=1, inplace=True)\n",
    "    print('[Variable Selection] 불필요한 컬럼을 제거하였습니다...')\n",
    "\n",
    "def handling_missing_values(df):\n",
    "    if df is None or df.empty:\n",
    "        print('타이타닉 데이터를 입력해주세요')\n",
    "        return None\n",
    "    age_median = df['Age'].median()\n",
    "    df['Age'].fillna(age_median, inplace=True)\n",
    "    print('[Handling Missing Values] 나이(Age)의 결측치를 중앙값(median)으로 채웠습니다...')\n",
    "\n",
    "def vectorization_sex(df):\n",
    "    if df is None or df.empty:\n",
    "        print('타이타닉 데이터를 입력해주세요')\n",
    "        return None\n",
    "    try:\n",
    "        sex_mapping = {'male': 0, 'female': 1}\n",
    "        df['Sex'].replace(sex_mapping, inplace=True)\n",
    "        print('[Vectorization] 성별(Age)을 처리하였습니다...')\n",
    "    except TypeError:\n",
    "        return df\n",
    "\n",
    "def vectorization_embarked(df):\n",
    "    if df is None or df.empty:\n",
    "        print('타이타닉 데이터를 입력해주세요')\n",
    "        return None\n",
    "    try:\n",
    "        embarked_mapping = {'S': 0, 'C': 1, 'Q': 2}\n",
    "        df['Embarked'].replace(embarked_mapping, inplace=True)\n",
    "        print('[Vectorization] 탑승한 곳(Embarked)을 처리하였습니다...')\n",
    "        return df.copy()\n",
    "    except TypeError:\n",
    "        return df\n",
    "\t\t\n",
    "def show_result(df):\n",
    "    sleep(2)\n",
    "    print('\\n\\n==========================')\n",
    "    print('>> 데이터 처리 결과는 다음과 같습니다.')\n",
    "    print(df.sample(8, random_state=623))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c1d66",
   "metadata": {},
   "source": [
    "타이타닉호가 어떻게 침몰됐는지 상황 파악하기\n",
    "첫 번째 시간에 생존자를 찾는 규칙을 만든 것을 기억하시나요?\n",
    "\n",
    "도메인 지식이 있으면 더 나은 데이터 준비를 할 수 있고, 더 나은 성능을 보이는 머신러닝 모델을 만들 수 있습니다.\n",
    "\n",
    "오른쪽에 소개되어 있는 영상을 시청하고,\n",
    "타이타닉호가 침몰됐을 때의 상황을 알아보세요.\n",
    "https://youtu.be/FSGeskFzE0s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72a4007",
   "metadata": {},
   "source": [
    "도메인 지식 늘리기\n",
    "깊은 도메인 지식을 가질수록 더 나은 데이터 준비(Data Preprocessing + Feature Engineering)\n",
    "가 가능해집니다.\n",
    "\n",
    "어떤 현상을 설명할 수 있는 더 많은 정보를 추가하고 적절한 Feature를 구성하게 된다면,\n",
    "\n",
    "머신러닝 프로젝트는 성공할 확률이 높아집니다.\n",
    "\n",
    "데이터 설명\n",
    "칼럼명\t설명\n",
    "survived\t생존 여부\n",
    "1(생존), 0(사망)\n",
    "pclass\t좌석 클래스\n",
    "1(1등석), 2(2등석), 3(3등석)\n",
    "name\t이름\n",
    "sex\t성별\n",
    "female(여성), male(남성)\n",
    "age\t나이\n",
    "sibsp\t함께 탑승한 형제 또는 배우자의 수\n",
    "parch\t함께 탑승한 부모 또는 자녀의 수\n",
    "ticket\t티켓 번호\n",
    "fare\t티켓 요금\n",
    "cabin\t선실 번호\n",
    "embarked\t탑승한 곳\n",
    "C(Cherbourg), Q(Queenstown), S(Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429c207",
   "metadata": {},
   "source": [
    "타이타닉 데이터 도메인 지식 늘리기\n",
    "이름(Name)의 중요성\n",
    "외국인들은 우리와 달리 이름에 많은 정보가 담겨 있습니다.\n",
    "이 사실을 몰랐던 준비 과정에서는 “이름”을 불필요한 칼럼이라고 생각하고 제거했습니다.\n",
    "타이타닉호 승선자들의 이름에는 Mr, Miss, Mrs, Master, Dr, Lady, Capt 등이 포함되어 있습니다.\n",
    "그 사람의 사회적 지위나 결혼 여부 등의 정보가 담겨있습니다.\n",
    "\n",
    "나이(Age)의 중요성 (1)\n",
    "이름을 통해 신분이나 결혼 여부를 알게된다면,\n",
    "약 20%의 사람들의 나이에 대한 결측치를 단순 ‘중앙값(median)’이 아니라\n",
    "좀 더 정확한 근삿값으로 채울 수 있게됩니다.\n",
    "\n",
    "나이(Age)의 중요성 (2)\n",
    "서양에서는 아이와 노인을 먼저 구출하는 성향이 있다고 합니다.\n",
    "10대, 20대가 아닌 연령군의 특성을 반영한다면\n",
    "단순 나이의 표현보다 나을 것 같습니다.\n",
    "\n",
    "가족의 중요성\n",
    "아래 타이타닉호 침몰을 분석한 인포그래픽을 보면,\n",
    "가족이 있을 때 생존확률이 높은 것을 발견하실 수 있습니다.\n",
    "생존자의 이름을 봤을 때 부부로 보이는 사람들이 함께 살아남는 경우도 발견할 수 있습니다.\n",
    "\n",
    "탑승위치의 중요성\n",
    "아래 그림을 보면 타이타닉이 어떻게 파손되고 침몰했는지 알 수 있습니다.\n",
    "탑승위치를 알 수 있다면 더 좋지 않을까요?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7dfd27",
   "metadata": {},
   "source": [
    "도메인 지식을 늘려\n",
    "데이터 준비하기\n",
    "도메인 지식이 없어도 Data Preprocessing을 할 수 있지만, 도메인 지식이 있으면 의미 있는 새로운 Feature를 생성할 수 있고 다양한 Feature Engineering Technique을 사용하여 머신러닝 모델의 성능을 향상시킬 수 있습니다.\n",
    "\n",
    "Feature Engineering 을 추가로 적용해서 모델의 결과가 어떻게 변하는지 직접 확인해 보겠습니다.\n",
    "\n",
    "이번 실습에서는 아래의 내용을 다룹니다.\n",
    "\n",
    "Data Preprocessing (이전 실습 참조)\n",
    "\n",
    "Feature Engineering\n",
    "\n",
    "※ Feature engineering을 하지 않았을 때의 모델의 정확도는 77.28%입니다.\n",
    "\n",
    "< switch 정보 >\n",
    "\n",
    "switch name\tdescription\t입력 가능한 값\n",
    "name\t탑승자의 이름에서\n",
    "사회적 지위나 결혼여부등의\n",
    "feature를 추출합니다\tTrue or False\n",
    "age\t다양한 맥락 정보를 활용하여\n",
    "결측치를 처리합니다\tTrue or False\n",
    "age_categorization\t새로운 연령군으로\n",
    "feature를 추출합니다\tTrue or False\n",
    "familysize\t가족구성원의 크기에 대한 feature를 생성합니다\tTrue or False\n",
    "\n",
    "지시사항\n",
    "1) Data Preprocessing을 실행시키세요\n",
    "이전 실습에서 다룬 Variable Selection, Handling Missing Value, Vectorization을 실행합니다.\n",
    "\n",
    "titanic = data_preprocessing(titanic)\n",
    "Copy\n",
    "2) Feature Engineering을 실행시키세요\n",
    "4개의 Feature Engineering 기법에 대해서 사용 여부를 체크하는 스위치(switch) 가 있습니다. 각 Feature Engineering 기법을 사용하고자 한다면 True를, 사용하지 않을 것이라면 False를 입력해주세요.\n",
    "\n",
    "switch = {\n",
    "    'name' :  False,\n",
    "    'age' :  False,\n",
    "    'age_categorization' :  False,\n",
    "    'familysize':  False \n",
    "}\n",
    "Copy\n",
    "3) 결과를 확인하고, 모델의 성능을 78% 이상으로 높여보세요.\n",
    "실행 버튼을 눌러 각 Feature Engineering 기법을 통해서 변하는 모델의 성능 결과를 확인하고, 모델의 성능을 78% 이상으로 높여 제출 버튼을 눌러보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b2827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main.py\n",
    "from with_domain import *\n",
    "\n",
    "\n",
    "def data_preparation_with_domain():\n",
    "    titanic = load_titanic_dataset()\n",
    "    \n",
    "    '''\n",
    "    [실습] 지시사항에 맞게 아래에 코드를 입력해주세요\n",
    "    \n",
    "    '''\n",
    "    # 1) Data Preprocessing을 실행시키세요 (아래 한 줄 코드 작성)\n",
    "    titanic = data_preprocessing(titanic)\n",
    "    \n",
    "    # 2) Feature Engineering을 실행시키세요 (아래 4개의 변수에 True or False 입력)\n",
    "    switch = {\n",
    "        'name' : True, # True or False\n",
    "        'age' : False, # True or False\n",
    "        'age_categorization' : False, #True or False\n",
    "        'familysize': False # True or False\n",
    "    }\n",
    "    \n",
    "    feature_engineering(titanic, switch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3) 결과를 확인하세요\n",
    "    show_result(titanic)\n",
    "    \n",
    "    return switch\n",
    "\t\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_preparation_with_domain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with_domain.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#import elice_utils\n",
    "from time import sleep\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "SEED=7020\n",
    "\n",
    "def load_titanic_dataset():\n",
    "    print('>>> 타이타닉 데이터셋을 불러옵니다...')\n",
    "    sleep(0.5)\n",
    "    return pd.read_csv('./data/titanic.csv')\n",
    "\n",
    "def feature_engineering(dataset, switch):\n",
    "    switch_name = switch['name']\n",
    "    switch_age = switch['age']\n",
    "    switch_age_categorization = switch['age_categorization']\n",
    "    switch_familysize = switch['familysize']\n",
    "\t\n",
    "    data = dataset[0]\n",
    "    feature_engineering_name(data, use=switch_name)\n",
    "    sleep(1.5)\n",
    "    feature_engineering_age(data, use_title=switch_name, use_age=switch_age)\n",
    "    sleep(1.5)\n",
    "    feature_engineering_age_categorization(data, use=switch_age_categorization)\n",
    "    sleep(1.5)\n",
    "    feature_engineering_familysize(data, use=switch_familysize)\n",
    "    sleep(1.5)\n",
    "    set_dtypes(data)\n",
    "\n",
    "def set_dtypes(df):\n",
    "    cat_cols = ['Sex', 'Pclass', 'Title', 'AgeC', 'Embarked']\n",
    "    num_cols = ['Age', 'Fare', 'FamilySize']\n",
    "    for col in df.columns:\n",
    "        if col in cat_cols:\n",
    "            df[col] = df[col].astype(np.object)\n",
    "        elif col in num_cols:\n",
    "            df[col] = df[col].astype(np.float)\n",
    "\n",
    "def feature_engineering_name(dataset_, use):\n",
    "    dataset_.is_copy = False\n",
    "    if use:\n",
    "        #train_test_data = [train_fe, test_fe] # combining train and test dataset\n",
    "\n",
    "        dataset_['Title'] = dataset_['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "        # Title 변수 0, 1, 2로 코딩\n",
    "        title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n",
    "                         \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n",
    "                         \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\n",
    "        dataset_['Title'] = dataset_['Title'].map(title_mapping)\n",
    "\n",
    "        # name 제거\n",
    "        dataset_.drop('Name', axis=1, inplace=True)\n",
    "        print('>>> feature_engineering_name \"Title\" faeture를 생성했습니다.')\n",
    "    else:\n",
    "        dataset_.drop('Name', axis=1, inplace=True)\n",
    "    #print('>>> 아래는 결과 샘플 입니다.')\n",
    "    #print(dataset_.head(1))\n",
    "\n",
    "        \n",
    "# Age의 결측값에 전체 데이터의 Median이 아닌 Title 그룹별 Median을 적용\n",
    "# use = False의 경우 전체 median 적용\n",
    "\n",
    "def feature_engineering_age(dataset_, use_title, use_age):\n",
    "    dataset_.is_copy = False\n",
    "    try:\n",
    "        if use_title and use_age:\n",
    "            # fill missing age with median age for each title (Mr, Mrs, Miss, Others)\n",
    "            dataset_['Age'].fillna(dataset_.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\n",
    "            print('>>> feature_engineering_age \"Age\"에 결측치를 Title 정보를 반영하여 채웠습니다.')\n",
    "        else:\n",
    "            dataset_['Age'].fillna(dataset_['Age'].median(), inplace = True)\n",
    "        #print('>>> 아래는 결과 샘플 입니다.')\n",
    "        #print(dataset_.head(1))\n",
    "\n",
    "    except KeyError:\n",
    "        print('Feature_engineering_age 수행 에러!')\n",
    "        print('Feature_engineering_name 수행 없이 사용할 수 없는 옵션입니다.')\n",
    "        \n",
    "def feature_engineering_age_categorization(dataset_, use):\n",
    "    dataset_.is_copy = False    \n",
    "    if use:\n",
    "        dataset_.loc[ dataset_['Age'] <= 16, 'AgeC'] = 0,\n",
    "        dataset_.loc[(dataset_['Age'] > 16) & (dataset_['Age'] <= 26), 'AgeC'] = 1,\n",
    "        dataset_.loc[(dataset_['Age'] > 26) & (dataset_['Age'] <= 36), 'AgeC'] = 2,\n",
    "        dataset_.loc[(dataset_['Age'] > 36) & (dataset_['Age'] <= 62), 'AgeC'] = 3,\n",
    "        dataset_.loc[ dataset_['Age'] > 62, 'AgeC'] = 4\n",
    "        print('>>> feature_engineering_age_categorization \"Age\"를 연령군 특성으로 그룹핑하였습니다.')\n",
    "    #print('>>> 아래는 결과 샘플 입니다.')\n",
    "    #print(dataset_.head(1))\n",
    "        \n",
    "def feature_engineering_familysize(dataset_, use):\n",
    "    dataset_.is_copy = False    \n",
    "    if use:\n",
    "        dataset_[\"FamilySize\"] = dataset_[\"SibSp\"] + dataset_[\"Parch\"] + 1\n",
    "        family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\n",
    "        dataset_['FamilySize'] = dataset_['FamilySize'].map(family_mapping)\n",
    "        dataset_.drop('SibSp', axis=1, inplace=True)\n",
    "        dataset_.drop('Parch', axis=1, inplace=True)\n",
    "        print('>>> feature_engineering_familysize \"FamilySize\" feature를 추가 생성하였습니다.')\n",
    "    #print('>>> 아래는 결과 샘플 입니다.')\n",
    "    #print(dataset_.head(1))\n",
    "\n",
    "        \n",
    "def data_preprocessing(dataset_):\n",
    "    dataset_.is_copy = False    \n",
    "    # Sex, Embarked 코딩\n",
    "    # male과 female을 0과 1로 코딩\n",
    "    sex_mapping = {\"male\": 0, \"female\": 1}\n",
    "    dataset_['Sex'] = dataset_['Sex'].map(sex_mapping)\n",
    "\n",
    "    # S = 0, C = 1, Q = 2로 코딩\n",
    "    embarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\n",
    "    dataset_['Embarked'] = dataset_['Embarked'].map(embarked_mapping)\n",
    "\n",
    "    # Ticket, PassengerId, Cabin 제거\n",
    "    try:\n",
    "        dataset_.drop(['Ticket', 'PassengerId', 'Cabin'], axis=1, inplace=True)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # Train data에서 Embarked가 없는 관측치 제거(2건)\n",
    "    dataset_.dropna(subset = ['Embarked'], inplace=True)\n",
    "\n",
    "    # 타겟값 분리\n",
    "    dataset_data = dataset_.drop('Survived', axis=1)\n",
    "    dataset_target = pd.DataFrame(data = dataset_['Survived'])\n",
    "\n",
    "    #print('Shape 확인 :', dataset_data.shape, dataset_target.shape)\n",
    "\n",
    "    return dataset_data, dataset_target\n",
    "\n",
    "def cleansing_categorical(indices_categorical_columns):\n",
    "    categorical_pipline =  Pipeline(steps=[\n",
    "                    ('select', FunctionTransformer(lambda data: data[:, indices_categorical_columns])),\n",
    "                    ('onehot', OneHotEncoder(sparse=False))\n",
    "                ])\n",
    "    return categorical_pipline\n",
    "\n",
    "def cleansing_numeric(indices_numeric_columns):\n",
    "    numeric_pipline = Pipeline(steps=[\n",
    "                    ('select', FunctionTransformer(lambda data: data[:, indices_numeric_columns])),\n",
    "                    ('scale', StandardScaler())\n",
    "                ])\n",
    "    return numeric_pipline\n",
    "\n",
    "def create_estimator(df, model):\n",
    "    indices_categorical_columns = df.dtypes == np.object\n",
    "    indices_numeric_columns = df.dtypes != np.object\n",
    "    if indices_categorical_columns.sum() != 0 and indices_numeric_columns.sum() != 0:\n",
    "        estimator = Pipeline(steps=[\n",
    "            ('cleansing', FeatureUnion(transformer_list=[\n",
    "                ('categorical', cleansing_categorical(indices_categorical_columns)),\n",
    "                ('numeric', cleansing_numeric(indices_numeric_columns))\n",
    "            ])),\n",
    "            ('modeling', model)\n",
    "        ])\n",
    "    elif indices_categorical_columns.sum() !=0 and indices_numeric_columns.sum() == 0:\n",
    "        estimator = Pipeline(steps=[\n",
    "            ('cleansing', FeatureUnion(transformer_list=[\n",
    "                ('categorical', cleansing_categorical(indices_categorical_columns))\n",
    "            ])),\n",
    "            ('modeling', model)\n",
    "        ])\n",
    "    elif indices_categorical_columns.sum() ==0 and indices_numeric_columns.sum() != 0:\n",
    "        estimator = Pipeline(steps=[\n",
    "            ('cleansing', FeatureUnion(transformer_list=[\n",
    "                ('numeric', cleansing_numeric(indices_numeric_columns))\n",
    "            ])),\n",
    "            ('modeling', model)\n",
    "        ])\n",
    "    else:\n",
    "        return None\n",
    "    return estimator\n",
    "\n",
    "def scoring_predicting(data, target):\n",
    "    model = DecisionTreeClassifier(random_state=SEED)\n",
    "    estimator = create_estimator(data, model)\n",
    "    score = cross_val_score(estimator=estimator, X=data.values, y=target, cv=5).mean()\n",
    "    predict = cross_val_predict(estimator=estimator, X=data.values, y=target, cv=5)\n",
    "    return score, predict\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "        #print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "def show_result(dataset):\n",
    "    score, predict = scoring_predicting(dataset[0], dataset[1].squeeze())\n",
    "    #cnf_matrix_ = confusion_matrix(y_true=target.squeeze(), y_pred=predict)\n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(cnf_matrix_, classes=['survived','died'],\n",
    "    #                      title='Confusion matrix_ Feature Engineering, without normalization')\n",
    "    print('==============================================')\n",
    "    #plt.savefig(\"image.svg\", format=\"svg\")\n",
    "    #elice_utils.send_image(\"image.svg\")\n",
    "    print('>> 최종 결과 Accuracy : {:.2f}%'.format(score*100))\n",
    "    return score\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
