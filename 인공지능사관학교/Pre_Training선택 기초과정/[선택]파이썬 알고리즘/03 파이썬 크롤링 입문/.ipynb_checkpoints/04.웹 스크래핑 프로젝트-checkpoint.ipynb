{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ad7499",
   "metadata": {},
   "source": [
    "목 차\n",
    "\n",
    "01. 프로젝트에 관하여\n",
    "\n",
    "02. 첫 번째 프로젝트\n",
    "\n",
    "03. 두 번째 프로젝트\n",
    "\n",
    "04. 세 번째 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f7f712",
   "metadata": {},
   "source": [
    "01. 프로젝트에 관하여"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace22a7",
   "metadata": {},
   "source": [
    "프로젝트?\n",
    "\n",
    "지금까지 배운 내용을 토대로 진행하는 실습 위주의 수업"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f598d0",
   "metadata": {},
   "source": [
    "어떤 프로젝트를 진행하나요?\n",
    "\n",
    "1. 간단한 페이지\n",
    "    - 다량의 데이터\n",
    "\n",
    "2. 검색 & Pagination\n",
    "    - 페이지시스템 사용하는 방법\n",
    "\n",
    "3. JS를 통한 동적 렌더링\n",
    "    - 자바스크립트를 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ed139d",
   "metadata": {},
   "source": [
    "프로젝트 수업의 목표\n",
    "\n",
    "나만의 Scraper를 만들 수 있는 실력 함양"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64100b2c",
   "metadata": {},
   "source": [
    "02. 첫 번째 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9434819",
   "metadata": {},
   "source": [
    "웹 페이지 소개\n",
    "\n",
    "전 세계 모든 국가의 수도, 인구, 면적 정보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e243cf0",
   "metadata": {},
   "source": [
    "프로젝트 개요\n",
    "\n",
    "웹 페이지 특징\n",
    "- 브라우저 진입 시, 모든 데이터가 한번에 로딩된다.\n",
    "- 대량의 데이터가 존재\n",
    "\n",
    "목표\n",
    "- 대량의 데이터가 로딩되기에, Wait 기능 필수\n",
    "- 데이터가 많을 때, 정제하는 방법 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ff7423",
   "metadata": {},
   "source": [
    "첫 번째 프로젝트\n",
    "첫 번째 프로젝트는 이 웹페이지에서 진행합니다.\n",
    "\n",
    "image\n",
    "\n",
    "해당 페이지에 있는 다량의 데이터를 추출하고, 이를 가공함으로써 지금껏 배운 Selenium 과 Python 을 복습해보겠습니다.\n",
    "\n",
    "\n",
    "지시사항\n",
    "웹에 있는 데이터를 구조화된 데이터(Structured Data)로 만들기 위해 class 를 먼저 정의합니다. 멤버 변수로 들어가야할 것은 다음과 같습니다.\n",
    "\n",
    "국가명\n",
    "수도\n",
    "인구\n",
    "면적\n",
    "국가별 정보가 담긴 요소를 모두 가져오고, 각 요소를 파이썬 class인 Country의 인스턴스로 만들어 country_list 에 추가합니다.\n",
    "\n",
    "모든 국가의 수도만 따로 list 를 만듭니다. 이 수도 목록을 sort() 또는 sorted() 를 이용하여 사전 순으로 정렬하고, 목록의 30번째 원소를 찾아 출력합니다.\n",
    "\n",
    "흔히들 60억 지구촌이라는데, 이 데이터에선 과연 어떨까요? 모든 국가의 인구를 sum() 을 이용하여 더해서 출력합니다.\n",
    "\n",
    "Tips!\n",
    "리스트에서 nnn 번째인 원소를 찾는 방법\n",
    "\n",
    "index는 000 부터 시작하므로, nnn 번째 원소의 index는 n−1n - 1n−1\n",
    "list[n - 1]\n",
    "면적 데이터는 아래 예시처럼 숫자가 아닌 표현으로 있을 수 있기 때문에, 그런 경우엔 변환을 해줘야 합니다.\n",
    "\n",
    "1.4e7 = 1.4 * 10^7 = 14,000,000\n",
    "Copy\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eaf8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "class Country:\n",
    "    # 지시사항 1번을 작성하세요.\n",
    "    # 웹에 있는 데이터를 구조화된 데이터(Structured Data)로 만들기 위해 \n",
    "    # class 를 먼저 정의합니다. \n",
    "    # 멤버 변수로 들어가야할 것은 다음과 같습니다.\n",
    "    # 국가명\n",
    "    # 수도\n",
    "    # 인구\n",
    "    # 면적\n",
    "    \n",
    "    # 생성자\n",
    "    def __init__(self, name, capital, population, area):\n",
    "        self.name = name\n",
    "        self.capital = capital\n",
    "        self.population = int(population)\n",
    "        if 'E' in area:\n",
    "            a, b = area.split('E') # a = 1.4 b = 7\n",
    "            self.area = float(a) * (10 ** int(b))\n",
    "        else:\n",
    "            self.area = float(area)\n",
    "\n",
    "\n",
    "\n",
    "with webdriver.Firefox() as driver:\n",
    "    driver.get(\"https://www.scrapethissite.com/pages/simple/\")\n",
    "\n",
    "    # 지시사항 2번을 작성하세요.\n",
    "    # 국가별 정보가 담긴 요소를 모두 가져오고, \n",
    "    # 각 요소를 파이썬 class인 Country의 인스턴스로 만들어 \n",
    "    # country_list 에 추가합니다.\n",
    "    country_list = []\n",
    "    div_list = driver.find_elements_by_class_name('country')\n",
    "    for div in div_list:\n",
    "        name = div.find_element_by_tag_name('h3').text\n",
    "        capital = div.find_element_by_class_name('country-capital').text\n",
    "        population = div.find_element_by_class_name('country-population').text\n",
    "        area = div.find_element_by_class_name('country-area').text\n",
    "        # 인스턴스 생성\n",
    "        country = Country(name , capital, population, area)\n",
    "        # 리스트에 추가\n",
    "        country_list.append(country)\n",
    "    \n",
    "    # 지시사항 3번을 작성하세요.\n",
    "    # 모든 국가의 수도만 따로 list 를 만듭니다. \n",
    "    # 이 수도 목록을 sort() 또는 sorted() 를 이용하여 사전 순으로 정렬하고,\n",
    "    # 목록의 30번째 원소를 찾아 출력합니다.\n",
    "    capital_list = []\n",
    "    for country in country_list:\n",
    "        capital_list.append(country.capital)\n",
    "    \n",
    "    capital_list.sort()\n",
    "    print(capital_list[29])\n",
    "    # 지시사항 4번을 작성하세요.\n",
    "    # 흔히들 60억 지구촌이라는데, \n",
    "    # 이 데이터에선 과연 어떨까요? \n",
    "    # 모든 국가의 인구를 sum() 을 이용하여 더해서 출력합니다.\n",
    "    pop_list = []\n",
    "    \n",
    "    for country in country_list:\n",
    "        pop_list.append(country.population)\n",
    "\n",
    "    print(sum(pop_list))\n",
    "\n",
    "\n",
    "    population_sum  = 0\n",
    "    for country in country_list:\n",
    "        population_sum += country.population\n",
    "    print(population_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe22fde",
   "metadata": {},
   "source": [
    "03. 두 번째 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e420ee89",
   "metadata": {},
   "source": [
    "웹 페이지 소개\n",
    "\n",
    "1990년 ~ 2011년 동안 미국 하키 팀의 성적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82c562",
   "metadata": {},
   "source": [
    "프로젝트 개요\n",
    "\n",
    "웹 페이지 특징\n",
    "    - 검색 기능이 있음\n",
    "    - Pagination을 통해 모든 데이터가 한 페이지에 담기지 않음\n",
    "    \n",
    "목표\n",
    "    - 검색 기능을 활용한 스크래핑을 할 수 있음\n",
    "    - 여러 페이지에 걸쳐 정보 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b9c8f1",
   "metadata": {},
   "source": [
    "두 번째 프로젝트\n",
    "두 번째 프로젝트는 이 웹페이지에서 진행합니다.\n",
    "\n",
    "image\n",
    "\n",
    "검색 기능을 활용해보고, 여러 page에 걸쳐 표시되고 있는 data를 추출하는 방법을 배웁니다.\n",
    "\n",
    "\n",
    "지시사항\n",
    "저번 실습과 마찬가지로, 웹에 있는 데이터를 구조화된 데이터(Structured Data)로 만들기 위해 class 를 먼저 정의합니다. 멤버 변수로 들어가야할 것은 다음과 같습니다.\n",
    "\n",
    "팀명\n",
    "기록연도\n",
    "승수\n",
    "패수\n",
    "검색 기능을 활용하기 위해, 단어를 입력할 요소와 Search 버튼 요소를 찾습니다.\n",
    "\n",
    "검색어를 입력(send_keys())하고 Search 버튼을 클릭(click())합니다. 검색어는 New 입니다.\n",
    "\n",
    "New로 검색하면 총 세 팀이 나올텐데, 연도별 각 팀의 기록을 Record 인스턴스로 만들어 record_list 에 저장합니다. 아마도 여러 page가 나올텐데, 아래의 Tips를 참고하여 모든 데이터를 불러올 수 있도록 합니다.\n",
    "\n",
    "record_list 를 이용하여 각 연도별 세 팀이 쌓은 승수의 합을 구해서 win_dict에 넣습니다. 해당 사전의 각 key는 연도, value는 승수입니다.\n",
    "\n",
    "승수를 가장 많이 쌓은 연도를 출력합니다.\n",
    "\n",
    "Tips!\n",
    "pagination되어 있는 모든 page의 url은 a 요소의 href 속성을 통해 알 수 있다.\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from typing import NamedTuple # namedtuple\n",
    "\n",
    "class Record(NamedTuple):\n",
    "    # 지시사항 1번을 작성하세요.\n",
    "    # 웹에 있는 데이터를 구조화된 데이터(Structured Data)로 만들기 위해\n",
    "    # class 를 먼저 정의합니다. \n",
    "    # 멤버 변수로 들어가야할 것은 다음과 같습니다.\n",
    "    # 팀명\n",
    "    # 기록연도\n",
    "    # 승수\n",
    "    # 패수\n",
    "    name: str\n",
    "    year: int\n",
    "    wins: int\n",
    "    losses: int\n",
    "    \n",
    "\n",
    "\n",
    "with webdriver.Firefox() as driver:\n",
    "    driver.get(\"https://www.scrapethissite.com/pages/forms/\")\n",
    "    # 지시사항 2번을 작성하세요.\n",
    "    # 검색 기능을 활용하기 위해, \n",
    "    # 단어를 입력할 요소와 Search 버튼 요소를 찾습니다.\n",
    "    # id는 하나의 html문서에서 고유한 값을 가진다 그렇게 짜는게 약속\n",
    "    input_e = driver.find_element_by_id('q')\n",
    "    # id가 없는 경우 class로도 찾을 수 있지만\n",
    "    # 반드시 하나만 있을 경우만 있는것이 아니기때문에\n",
    "    # xpath로 찾아준다.\n",
    "    search_e = driver.find_element_by_xpath(\n",
    "    '//*[@id=\"hockey\"]/div/div[4]/div/form/input[2]')\n",
    "\n",
    "    # 지시사항 3번을 작성하세요.\n",
    "    # 검색어를 입력(send_keys())하고 \n",
    "    # Search 버튼을 클릭(click())합니다. 검색어는 New 입니다.\n",
    "    input_e.send_keys(\"New\")\n",
    "    search_e.click()\n",
    "    \n",
    "    # 지시사항 4번을 작성하세요.\n",
    "    # New로 검색하면 총 세 팀이 나올텐데, \n",
    "    # 연도별 각 팀의 기록을 Record 인스턴스로 만들어\n",
    "    # record_list 에 저장합니다. \n",
    "    # 아마도 여러 page가 나올텐데, \n",
    "    # 아래의 Tips를 참고하여 모든 데이터를 불러올 수 있도록 합니다.\n",
    "    record_list = []\n",
    "    ul = driver.find_element_by_class_name(\"pagination\")\n",
    "    a_list = ul.find_elements_by_tag_name(\"a\")\n",
    "    \n",
    "    url_list = []\n",
    "    # 다음페이지로 가는 링크 제거(마지막 인덱스)\n",
    "    for a in a_list[:-1]:\n",
    "        url_list.append(a.get_attribute('href'))\n",
    "    #print(url_list)\n",
    "    \n",
    "    for url in url_list:\n",
    "        driver.get(url)\n",
    "        \n",
    "        tbody = driver.find_element_by_tag_name('tbody')\n",
    "        team_list = tbody.find_elements_by_class_name('team')\n",
    "        \n",
    "        for team in team_list:\n",
    "            name = team.find_element_by_class_name('name').text\n",
    "            year = team.find_element_by_class_name('year').text\n",
    "            wins = team.find_element_by_class_name('wins').text\n",
    "            losses = team.find_element_by_class_name('losses').text\n",
    "            record = Record(name = name,\n",
    "                            year = int(year),\n",
    "                            wins = int(wins),\n",
    "                            losses = int(losses))\n",
    "            #print(record)\n",
    "            record_list.append(record)\n",
    "        \n",
    "    # 지시사항 5번을 작성하세요.\n",
    "    # record_list 를 이용하여 \n",
    "    # 각 연도별 세 팀이 쌓은 승수의 합을 구해서 win_dict에 넣습니다. \n",
    "    # 해당 사전의 각 key는 연도, value는 승수입니다.\n",
    "    win_dict = {}  # {1990: 100, 1991: 110, 1992: 120, ...}\n",
    "    for record in record_list:\n",
    "        if record.year not in win_dict:\n",
    "            win_dict[record.year] = 0\n",
    "        win_dict[record.year] += record.wins\n",
    "    #print(win_dict)\n",
    "    # 지시사항 6번을 작성하세요.\n",
    "    # 승수를 가장 많이 쌓은 연도를 출력합니다.\n",
    "    best_year = 1990\n",
    "    for year in win_dict:\n",
    "        if win_dict[year] > win_dict[best_year]:\n",
    "            best_year = year\n",
    "        \n",
    "    print(best_year)\n",
    "    # print(win_dict[best_year])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791725e3",
   "metadata": {},
   "source": [
    "04. 세 번째 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef26c875",
   "metadata": {},
   "source": [
    "웹 페이지 소개\n",
    "\n",
    "연도별, 작품별 오스카 상 수상 목록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a70fc",
   "metadata": {},
   "source": [
    "프로젝트 개요\n",
    "\n",
    "웹 페이지 특징\n",
    "    - 처음엔 데이터가 전혀 없음\n",
    "    - 버튼을 누르면 데이터를 ajax통신으로 가져오고,\n",
    "      이를 웹페이지에 보여줌\n",
    "   #ajax : JS이용 서버에 데이터 통신하여 데이터를 받아옴\n",
    "      \n",
    "목표\n",
    "    - 버튼을 눌러야만 표시되는 데이터를 추출할수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400347e8",
   "metadata": {},
   "source": [
    "세 번째 프로젝트\n",
    "세 번째 프로젝트는 이 웹페이지에서 진행합니다.\n",
    "\n",
    "image\n",
    "\n",
    "서버와 ajax 통신 이 이루어지는 페이지이므로, wait 기능을 활용해보고, 여러 페이지로부터 데이터를 추출하는 방법을 학습합니다.\n",
    "\n",
    "\n",
    "지시사항\n",
    "저번 실습과 마찬가지로, 웹에 있는 데이터를 구조화된 데이터(Structured Data)로 만들기 위해 class 를 먼저 정의합니다. 멤버 변수로 들어가야할 것은 다음과 같습니다.\n",
    "\n",
    "영화 제목\n",
    "해당년도\n",
    "후보등록 수\n",
    "수상 개수\n",
    "각 연도별 링크 요소를 찾습니다.\n",
    "\n",
    "각 연도별 링크 요소를 하나씩 클릭해가며 모든 영화 데이터를 추출해서 Film 인스턴스로 만들어 film_list 에 추가합니다.\n",
    "\n",
    "그런데 버튼을 클릭하면 데이터를 서버에서 가져오는 ajax 통신이 이루어집니다. 때문에 로딩에 시간이 걸리게 되어 Wait 기능이 필수적입니다. 이 실습에 적절한 Wait 기능을 추가해서 데이터 추출을 해봅시다.\n",
    "\n",
    "film_list 를 이용해서, 10개 이상 후보에 올랐던 작품의 작품명을 사전순으로 정렬하고, 모두 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60893710",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'chromedriver' executable needs to be in PATH. Please see https://chromedriver.chromium.org/home\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_line_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             self.process = subprocess.Popen(cmd, env=self.env,\n\u001b[0m\u001b[0;32m     72\u001b[0m                                             \u001b[0mclose_fds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'Windows'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 858\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    859\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1310\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1311\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1312\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-de83a16d70ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.scrapethissite.com/pages/ajax-javascript/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mservice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mService\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mservice_log_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         super(WebDriver, self).__init__(DesiredCapabilities.CHROME['browserName'], \"goog\",\n\u001b[0m\u001b[0;32m     71\u001b[0m                                         \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                                         \u001b[0mservice_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesired_capabilities\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\common\\service.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                 raise WebDriverException(\n\u001b[0m\u001b[0;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m     83\u001b[0m                         os.path.basename(self.path), self.start_error_message)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: 'chromedriver' executable needs to be in PATH. Please see https://chromedriver.chromium.org/home\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "class Film(NamedTuple):\n",
    "    # 지시사항 1번을 작성하세요.\n",
    "    # 데이터(Structured Data)로 만들기 위해 \n",
    "    # class 를 먼저 정의합니다. \n",
    "    # 멤버 변수로 들어가야할 것은 다음과 같습니다.\n",
    "    # 영화 제목\n",
    "    # 해당년도\n",
    "    # 후보등록 수\n",
    "    # 수상 개수\n",
    "    name : str\n",
    "    year : int\n",
    "    nominations : int\n",
    "    awards : int\n",
    "\n",
    "\n",
    "\n",
    "with webdriver.Chrome() as driver:\n",
    "    driver.get(\"https://www.scrapethissite.com/pages/ajax-javascript/\")\n",
    "\n",
    "    # 지시사항 2번을 작성하세요.\n",
    "    # 각 연도별 링크 요소를 찾습니다.\n",
    "    a_list = driver.find_elements_by_class_name('year-link')\n",
    "    \n",
    "    # 지시사항 3번과 4번을 작성하세요.\n",
    "    film_list = list()\n",
    "    # 각 연도별 링크 요소를 하나씩 클릭해가며 \n",
    "    # 모든 영화 데이터를 추출해서 Film 인스턴스로 만들어 \n",
    "    # film_list 에 추가합니다.\n",
    "    for a in a_list:\n",
    "        a.click()\n",
    "        # 그런데 버튼을 클릭하면 \n",
    "        # 데이터를 서버에서 가져오는 ajax 통신이 이루어집니다. \n",
    "        # 때문에 로딩에 시간이 걸리게 되어 \n",
    "        # Wait 기능이 필수적입니다. \n",
    "        # 이 실습에 적절한 Wait 기능을 추가해서 데이터 추출을 해봅시다.\n",
    "        time.sleep(5)\n",
    "        tbody = driver.find_element_by_tag_name('tbody')\n",
    "        tr_list = tbody.find_elements_by_tag_name('tr')\n",
    "        \n",
    "        for tr in tr_list:\n",
    "            film_list.append(Film\n",
    "                (name = tr.find_element_by_class_name('film-title').text,\n",
    "                 year = int(a.text),\n",
    "                 nominations = int(tr.find_element_by_class_name('film-nominations').text),\n",
    "                 awards=int(tr.find_element_by_class_name('film-awards').text))\n",
    "                 )\n",
    "                 \n",
    "        \n",
    "    # 지시사항 5번을 작성하세요.\n",
    "    # film_list 를 이용해서, \n",
    "    # 10개 이상 후보에 올랐던 작품의 작품명을 \n",
    "    # 사전순으로 정렬하고, 모두 출력해봅시다.\n",
    "    many_nomination_list = list()\n",
    "    for film in film_list:\n",
    "        if film.nominations >= 10:\n",
    "            many_nomination_list.append(film.name)\n",
    "            \n",
    "    print(sorted(many_nomination_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220703c",
   "metadata": {},
   "source": [
    "웹 스크래핑 프로젝트\n",
    "파이썬 크롤링 입문 과정을 수강 완료하신 걸 진심으로 축하드립니다 🤗\n",
    "이제 모두 파이썬 크롤링 마스터가 되셨겠군요ㅎㅎ\n",
    "이번 장의 프로젝트를 되돌아보고 마치도록 하겠습니다!\n",
    "\n",
    "\n",
    "\n",
    "1. 첫 번째 프로젝트\n",
    "\n",
    "웹 페이지 특징\n",
    "\n",
    "브라우저 진입 시, 모든 데이터가 한 번에 로딩\n",
    "대량의 데이터가 존재\n",
    "목표\n",
    "\n",
    "대량의 데이터가 로딩되기에 Wait 기능 필수\n",
    "데이터가 많을 때, 정제하는 방법 학습\n",
    "from selenium import webdriver\n",
    "\n",
    "class Country:\n",
    "    # 웹에 있는 데이터를 구조화된 데이터로 만들기\n",
    "    def __init__(self, name, capital, population, area):\n",
    "        self.name = name\n",
    "        self.capital = capital\n",
    "        self.population = population\n",
    "        self.area = area\n",
    "\n",
    "with webdriver.Firefox() as driver:\n",
    "    driver.get(\"https://www.scrapethissite.com/pages/simple/\")\n",
    "    \n",
    "    # 국가별 정보가 담긴 요소를 모두 가져와서 country_list에 추가하기\n",
    "    country_list = []\n",
    "    div_list = driver.find_elements_by_xpath(\n",
    "        '//div[@class=\"row\"]/div[contains(@class, \"country\")]'\n",
    "    )\n",
    "    \n",
    "    for div in div_list:\n",
    "        population = int(div.find_element_by_class_name(\"country-population\").text)\n",
    "        area = div.find_element_by_class_name(\"country-area\").text\n",
    "        \n",
    "        country = Country(\n",
    "            name = div.find_element_by_tag_name(\"h3\").text,\n",
    "            capital = div.find_element_by_class_name(\"country-capital\").text,\n",
    "            population = population,\n",
    "            area = area,\n",
    "        )\n",
    "        \n",
    "        country_list.append(country)\n",
    "    ...\n",
    "\n",
    "\n",
    "2. 두 번째 프로젝트\n",
    "\n",
    "웹 페이지 특징\n",
    "\n",
    "검색 기능이 있음\n",
    "Pagination을 통해 모든 데이터가 한 페이지에 담기지 않음\n",
    "목표\n",
    "\n",
    "검색 기능을 활용한 스크래핑을 할 수 있음\n",
    "여러 페이지에 걸쳐 정보 추출\n",
    "from typing import NamedTuple\n",
    "from selenium import webdriver\n",
    "\n",
    "class Record(NamedTuple):\n",
    "    # 웹에 있는 데이터를 구조화된 데이터로 만들기\n",
    "    name: str\n",
    "    year: int\n",
    "    wins: int\n",
    "    losses: int\n",
    "\n",
    "with webdriver.Firefox() as driver:\n",
    "    driver.get(\"https://www.scrapethissite.com/pages/forms/\")\n",
    "    \n",
    "    # 검색 기능 활용하기 (단어 입력 요소와 Search 버튼 요소 찾기)\n",
    "    input_e = driver.find_element_by_id(\"q\")\n",
    "    search_e = driver.find_element_by_xpath(\n",
    "        '//*[@id=\"hockey\"]/div/div[4]/div/form/input[2]'\n",
    "    )\n",
    "    \n",
    "    # 검색어 입력하고 Search 버튼 클릭하기\n",
    "    input_e.send_keys(\"New\") # 검색어는 New\n",
    "    search_e.click()\n",
    "    \n",
    "    # 검색 결과 저장하기\n",
    "    record_list = []\n",
    "    ul = driver.find_element_by_class_name(\"pagination\")\n",
    "    a_list = ul.find_elements_by_tag_name(\"a\")\n",
    "    \n",
    "    url_list = []\n",
    "    for a in a_list:\n",
    "        url_list.append(a.get_attribute(\"href\"))\n",
    "    \n",
    "    for i in range(len(url_list)-1):\n",
    "        driver.get(url_list[i])\n",
    "        \n",
    "        tbody = driver.find_element_by_tag_name(\"tbody\")\n",
    "        team_list = tbody.find_elements_by_class_name(\"team\")\n",
    "        \n",
    "        for team in  team_list:\n",
    "            name = team.find_element_by_class_name(\"name\").text\n",
    "            year = team.find_element_by_class_name(\"year\").text\n",
    "            year = int(year)\n",
    "            wins = team.find_element_by_class_name(\"wins\").text\n",
    "            wins = int(wins)\n",
    "            losses = team.find_element_by_class_name(\"losses\").text\n",
    "            losses = int(losses)\n",
    "            \n",
    "            record_list.append(Record(name=name, year=year, wins=wins, losses=losses))\n",
    "    ...\n",
    "\n",
    "\n",
    "3. 세 번째 프로젝트\n",
    "\n",
    "웹 페이지 특징\n",
    "\n",
    "처음엔 데이터가 전혀 없음\n",
    "버튼을 누르면 데이터를 ajax 통신으로 가져오고, 이를 웹페이지에 보여줌\n",
    "목표\n",
    "\n",
    "버튼을 눌러야만 표시되는 데이터를 추출할 수 있음\n",
    "import time\n",
    "from typing import NamedTuple\n",
    "from selenium import webdriver\n",
    "\n",
    "class Film(NamedTuple):\n",
    "    # 웹에 있는 데이터를 구조화된 데이터로 만들기\n",
    "    year: int\n",
    "    name: str\n",
    "    nominations: int\n",
    "    awards: int\n",
    "\n",
    "with webdriver.Firefox() as driver:\n",
    "    driver.get(\"https://www.scrapethissite.com/pages/ajax-javascript/\")\n",
    "    \n",
    "    # 각 연도별 링크 요소 찾기\n",
    "    a_list = driver.find_elements_by_class_name(\"year-link\")\n",
    "    \n",
    "    # 영화 데이터 추출하여 film_list에 추가하기\n",
    "    # 이 때 ajax 통신이 이루어지기 때문에 적절한 Wait 기능을 추가하기\n",
    "    film_list = list()\n",
    "    \n",
    "    for a in a_list:\n",
    "        a.click()\n",
    "        time.sleep(3)\n",
    "        \n",
    "        tbody = driver.find_element_by_tag_name(\"tbody\")\n",
    "        tr_list = tbody.find_elements_by_tag_name(\"tr\")\n",
    "        \n",
    "        for tr in tr_list:\n",
    "            film_list.append(\n",
    "                Film(\n",
    "                    name = tr.find_element_by_class_name(\"film-title\").text,\n",
    "                    year = int(a.text),\n",
    "                    nominations = int(tr.find_element_by_class_name(\"film-nominations\").text),\n",
    "                    awards = int(tr.find_element_by_class_name(\"film-awards\").text),\n",
    "                )\n",
    "            )\n",
    "    ...\n",
    "\n",
    "course\n",
    "04 웹 스크래핑 프로젝트\n",
    "4장을 닫으며\n",
    "\n",
    "이전\n",
    "11\n",
    "/12\n",
    "\n",
    "다음\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
